# üî• Pipeline de An√°lise e Monitoramento de Focos de Queimadas 

## üíª Sobre o Projeto

O **Pipeline de An√°lise e Monitoramento de Focos de Queimadas** consiste em uma infraestrutura de dados (Data Pipeline) projetada para automatizar a ingest√£o, o processamento e a consolida√ß√£o de dados hist√≥ricos de focos de queimadas (inc√™ndios florestais).

Esta aplica√ß√£o foi desenvolvida como parte da atividade pr√°tica para demonstrar a implementa√ß√£o at√© camada **Silver** de um Data Lake (Lakehouse/Medallion Architecture) em um ambiente simulado. O objetivo final √© criar uma base de dados limpa, particionada e consolidada em formato parquet, que servir√° como fonte de dados confi√°vel para an√°lises geoespaciais e relat√≥rios.

### üîó Arquitetura Implementada 

O pipeline implementado no ambiente simulado (Google Colab) estabelece a camada **Bronze** do Data Lake com as seguintes caracter√≠sticas:

| Etapa | L√≥gica Implementada |
| :--- | :--- |
| **Ingest√£o** | C√≥pia de arquivos CSV de uma fonte externa (Drive) para o ambiente de processamento. |
| **Armazenamento Raw** | Estrutura de particionamento hier√°rquico por data (`ano={yyyy}/mes={mm}`) para os CSVs originais. |
| **Transforma√ß√£o**| Concatenamento de DataFrames, enriquecimento com metadados (`ano`, `mes`) e aplica√ß√£o de **impen√™ncia** (remo√ß√£o de duplicatas). |
| **Armazenamento Final**  | Salvamento do conjunto de dados limpo e consolidado no formato **Parquet** e CSV. |

### üõ† Tecnologias Utilizadas

| Camada | Tecnologias Atuais (Open Source/Simuladas) |
| :--- | :--- |
| **Linguagem/Processamento** | Python, Pandas, Dask, cuDF |
| **Ambiente/Orquestra√ß√£o** | Google Colab (Execu√ß√£o manual/interativa) |
| **Armazenamento (Fonte/Destino)** | Google Drive (Fonte), File System do Colab (Destino), CSV, Parquet |

<img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="python"/> &nbsp; <img src="https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white" alt="Pandas"/> &nbsp;  <img src="https://img.shields.io/badge/Dask-F8766D?style=for-the-badge&logo=dask&logoColor=white" alt="Dask"/> &nbsp; <img src="https://img.shields.io/badge/cuDF-7097C2?style=for-the-badge&logo=nvidia&logoColor=white" alt="cuDF"/>

### üöÄ Sugest√µes de Refinamento (Tecnologias Pagas/Gerenciadas na AWS)

Para levar o pipeline, que j√° utiliza acelera√ß√£o por GPU (cuDF/Dask-cuDF), a um ambiente de produ√ß√£o escal√°vel e robusto, sugerimos a migra√ß√£o para o ecossistema **Amazon Web Services (AWS)**:

* **Armazenamento e Data Lakehouse (S3 & Athena/Redshift):**
    * O armazenamento central (Data Lake) deve ser persistido e escalado no **Amazon S3 (Simple Storage Service)**, ideal para armazenar os arquivos Parquet.
    * O destino anal√≠tico deve ser o **Amazon Athena** (consultas *serverless* diretamente no S3) ou o **Amazon Redshift** (Data Warehouse), implementando a arquitetura *Lakehouse*.

* **Orquestra√ß√£o e Automa√ß√£o (MWAA & Step Functions):**
    * O agendamento robusto e o monitoramento do *workflow* ser√£o feitos com o **Amazon Managed Workflows for Apache Airflow (MWAA)**, mantendo a flexibilidade do Airflow.
    * Para *workflows* mais espec√≠ficos ou *serverless*, pode-se utilizar **AWS Step Functions**.

* **Processamento Acelerado em Produ√ß√£o (AWS EMR & ECS/SageMaker):**
    * O processamento distribu√≠do em escala ser√° garantido pelo **Amazon EMR** configurado para rodar *clusters* **Apache Spark** (sem MapReduce).
    * Para manter a acelera√ß√£o por GPU (Dask-cuDF/RAPIDS) em produ√ß√£o, sugere-se a utiliza√ß√£o do **Amazon ECS (Elastic Container Service)** ou **Amazon SageMaker**, executando *containers* em inst√¢ncias **EC2** otimizadas com GPUs dedicadas (fam√≠lia P3/P4), garantindo a alta performance alcan√ßada no Colab.

<img src="https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white" alt="AWS"/> &nbsp; <img src="https://img.shields.io/badge/Amazon_S3-569A31?style=for-the-badge&logo=amazons3&logoColor=white" alt="Amazon S3"/> &nbsp; <img src="https://img.shields.io/badge/Amazon_Redshift-C02A36?style=for-the-badge&logo=amazonredshift&logoColor=white" alt="Amazon Redshift"/> &nbsp; <img src="https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=Apache-Airflow&logoColor=white" alt="Apache Airflow"/> &nbsp; <img src="https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white" alt="Apache Spark"/>

***

## üóÇ Como rodar o projeto (Ambiente Simulador)

As instru√ß√µes a seguir pressup√µem a execu√ß√£o no ambiente interativo do Google Colab:

```bash
# 1. Carregar o arquivo 'queimadas.ipynb' no Google Colab.
# 2. Executar as c√©lulas de setup e importa√ß√µes iniciais.

# 3. Montar o Google Drive para acessar a fonte de dados simulada (CSV):
# from google.colab import drive
# drive.mount('/content/drive')

# 4. Executar o pipeline de Ingest√£o:
#    - Cria√ß√£o da estrutura de pastas de destino.
#    - C√≥pia dos CSVs da fonte para a pasta Bronze Raw particionada.

# 5. Executar o pipeline de Transforma√ß√£o:
#    - Leitura e concatena√ß√£o dos DataFrames.
#    - Aplica√ß√£o da l√≥gica de limpeza e remo√ß√£o de duplicatas.

# 6. Executar o pipeline de Armazenamento:
#    - Salvamento do DataFrame consolidado no formato CSV e Parquet.
```
## üìù Documento de Arquitetura

### Fonte dos Dados

| Campo | Conte√∫do |
| :--- | :--- |
| **Fonte de Dados** | Arquivos CSV de focos de queimadas (Ex: dados do INPE/Monitoramento). |
| **Localiza√ß√£o da Fonte** | Google Drive (Simula√ß√£o de fonte externa) |
| **Formato de Entrada** | CSV (Comma Separated Values) |

***

## ‚úÖ Checklist do Estado Atual

A implementa√ß√£o atual no `queimadas.ipynb` reflete a conclus√£o das seguintes fases do pipeline:

| Parte do Pipeline | Estado Atual (Ambiente Colab) |
| :--- | :--- |
| **Ingest√£o** | **(x) Finalizado** (C√≥pia da fonte para o Bronze Raw) |
| **Armazenamento** | **(x) Finalizado** (Estrutura de Data Lake Bronze implementada) |
| **Transforma√ß√£o** | **(x) Finalizado** (Limpeza b√°sica, metadados e deduplica√ß√£o aplicadas) |

***

## üöÄ Equipe e Divis√£o de Tarefas

| Membro da Equipe | Fun√ß√£o | Tarefas e Responsabilidades |
| :--- | :--- | :--- |
| **Julio Padilha** | **Engenheiro de dados(Otimiza√ß√£o e Escalabilidade do Pipeline)** | Expandiu o dataset de 1 para 22 meses, realizando a concatena√ß√£o e integra√ß√£o de arquivos. Implementou solu√ß√µes de otimiza√ß√£o de desempenho e mem√≥ria com Dask e cuDF, aproveitando o processamento paralelo e o uso da GPU do Colab. |
| **Matheus Bione** | **Suporte T√©cnico** | Verificando se os dados transformados mant√™m integridade em rela√ß√£o √† ingest√£o original. Realizou a organiza√ß√£o de diret√≥rios, limpeza de arquivos duplicados e padroniza√ß√£o de nomes dentro do projeto. |
| **Nicole Victory** | **Analista de dados(valida√ß√£o e qualidade dos dados)** | Criou scripts para verifica√ß√£o e limpeza dos datasets ap√≥s a ingest√£o na camada Bronze, garantindo que os arquivos contenham as colunas esperadas e sem valores nulos cr√≠ticos. Gerou relat√≥rios autom√°ticos de estat√≠sticas e qualidade dos dados (profiling) para documenta√ß√£o e an√°lise. |
| **Roberto Arruda** | **Cientista de Dados (Ingest√£o e Modelagem)** |  Realizou a ingest√£o inicial dos dados, estruturando o pipeline nas camadas Bronze, Silver e Gold. Foi respons√°vel pela organiza√ß√£o da arquitetura de pastas, padroniza√ß√£o do fluxo de dados e pela cria√ß√£o das primeiras transforma√ß√µes entre as camadas. |


***

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa [MIT](https://opensource.org/licenses/MIT).
