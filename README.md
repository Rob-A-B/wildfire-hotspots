# üî• Pipeline de An√°lise e Monitoramento de Focos de Queimadas 

## üíª Sobre o Projeto

O **Pipeline de An√°lise e Monitoramento de Focos de Queimadas** consiste em uma infraestrutura de dados (Data Pipeline) projetada para automatizar a ingest√£o, o processamento e a consolida√ß√£o de dados hist√≥ricos de focos de queimadas (inc√™ndios florestais).

Esta aplica√ß√£o foi desenvolvida como parte da atividade pr√°tica para demonstrar a implementa√ß√£o da camada **Bronze** de um Data Lake (Lakehouse/Medallion Architecture) em um ambiente simulado. O objetivo final √© criar uma base de dados limpa, particionada e consolidada em formato parquet, que servir√° como fonte de dados confi√°vel para an√°lises geoespaciais e relat√≥rios.

### üîó Arquitetura Implementada (Camada Bronze)

O pipeline implementado no ambiente simulado (Google Colab) estabelece a camada **Bronze** do Data Lake com as seguintes caracter√≠sticas:

| Etapa | Fluxo de Dados | L√≥gica Implementada |
| :--- | :--- | :--- |
| **Ingest√£o** | Google Drive (Fonte) ‚Üí Bronze Raw | C√≥pia de arquivos CSV de uma fonte externa (Drive) para o ambiente de processamento. |
| **Armazenamento Raw** | Bronze Raw | Estrutura de particionamento hier√°rquico por data (`ano={yyyy}/mes={mm}`) para os CSVs originais. |
| **Transforma√ß√£o** | Bronze Raw ‚Üí Bronze Current | Concatenamento de DataFrames, enriquecimento com metadados (`ano`, `mes`) e aplica√ß√£o de **impen√™ncia** (remo√ß√£o de duplicatas). |
| **Armazenamento Final** | Bronze Current | Salvamento do conjunto de dados limpo e consolidado no formato **Parquet** e CSV. |

### üõ† Tecnologias Utilizadas

| Camada | Tecnologias Atuais (Open Source/Simuladas) |
| :--- | :--- |
| **Linguagem/Processamento** | Python, Pandas |
| **Ambiente/Orquestra√ß√£o** | Google Colab (Execu√ß√£o manual/interativa) |
| **Armazenamento (Fonte/Destino)** | Google Drive (Fonte), File System do Colab (Destino), CSV, Parquet |

<img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="python"/> &nbsp; <img src="https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white" alt="Pandas"/> &nbsp; <img src="https://img.shields.io/badge/apache%20parquet-2A62AA?style=for-for-the-badge&logo=apache&logoColor=white" alt="Parquet"/>

### üöÄ Sugest√µes de Refinamento (Tecnologias Pagas/Gerenciadas)

Para levar este pipeline a um ambiente de produ√ß√£o escal√°vel e robusto, sugerimos a migra√ß√£o para o ecossistema Google Cloud Platform (GCP):

* **Armazenamento Central:** **Google Cloud Storage (GCS)** para o Data Lake e **Google BigQuery** para o Data Warehouse anal√≠tico.
* **Orquestra√ß√£o:** **Google Cloud Composer** (Apache Airflow Gerenciado) para agendamento, monitoramento e gest√£o do fluxo de trabalho.
* **Processamento em Escala:** **Google Cloud Dataproc** (Apache Spark Gerenciado) para processamento distribu√≠do de grandes volumes de dados.

<img src="https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white" alt="Google Cloud"/> &nbsp; <img src="https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=Apache-Airflow&logoColor=white" alt="Apache Airflow"/> &nbsp; <img src="https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white" alt="Apache Spark"/>

***

## üóÇ Como rodar o projeto (Ambiente Simulador)

As instru√ß√µes a seguir pressup√µem a execu√ß√£o no ambiente interativo do Google Colab:

```bash
# 1. Carregar o arquivo 'queimadas.ipynb' no Google Colab.
# 2. Executar as c√©lulas de setup e importa√ß√µes iniciais.

# 3. Montar o Google Drive para acessar a fonte de dados simulada (CSV):
# from google.colab import drive
# drive.mount('/content/drive')

# 4. Executar o pipeline de Ingest√£o:
#    - Cria√ß√£o da estrutura de pastas de destino (Bronze Raw, Bronze Current).
#    - C√≥pia dos CSVs da fonte para a pasta Bronze Raw particionada.

# 5. Executar o pipeline de Transforma√ß√£o:
#    - Leitura e concatena√ß√£o dos DataFrames.
#    - Aplica√ß√£o da l√≥gica de limpeza e remo√ß√£o de duplicatas.

# 6. Executar o pipeline de Armazenamento:
#    - Salvamento do DataFrame consolidado no formato CSV e Parquet.
```
## üìù Documento de Arquitetura

### Fonte dos Dados

| Campo | Conte√∫do |
| :--- | :--- |
| **Fonte de Dados** | Arquivos CSV de focos de queimadas (Ex: dados do INPE/Monitoramento). |
| **Localiza√ß√£o da Fonte** | Google Drive (Simula√ß√£o de fonte externa) |
| **Formato de Entrada** | CSV (Comma Separated Values) |

***

## ‚úÖ Checklist do Estado Atual

A implementa√ß√£o atual no `queimadas.ipynb` reflete a conclus√£o das seguintes fases do pipeline:

| Parte do Pipeline | Estado Atual (Ambiente Colab) |
| :--- | :--- |
| **Ingest√£o** | **(x) Finalizado** (C√≥pia da fonte para o Bronze Raw) |
| **Armazenamento** | **(x) Finalizado** (Estrutura de Data Lake Bronze implementada) |
| **Transforma√ß√£o** | **(x) Finalizado** (Limpeza b√°sica, metadados e deduplica√ß√£o aplicadas) |

***

## üöÄ Equipe e Divis√£o de Tarefas

| Membro da Equipe | Fun√ß√£o | Tarefas e Responsabilidades |
| :--- | :--- | :--- |
| **Julio Padilha** | **Engenheiro de dados(Otimiza√ß√£o e Escalabilidade do Pipeline)** | Expandiu o dataset de 1 para 22 meses, realizando a concatena√ß√£o e integra√ß√£o de arquivos. Implementou solu√ß√µes de otimiza√ß√£o de desempenho e mem√≥ria com Dask e cuDF, aproveitando o processamento paralelo e o uso da GPU do Colab. |
| **Matheus Bione** | **Suporte T√©cnico** | Verificando se os dados transformados mant√™m integridade em rela√ß√£o √† ingest√£o original. Realizou a organiza√ß√£o de diret√≥rios, limpeza de arquivos duplicados e padroniza√ß√£o de nomes dentro do projeto. |
| **Nicole Victory** | **Analista de dados(valida√ß√£o e qualidade dos dados)** | Criou scripts para verifica√ß√£o e limpeza dos datasets ap√≥s a ingest√£o na camada Bronze, garantindo que os arquivos contenham as colunas esperadas e sem valores nulos cr√≠ticos. Gerou relat√≥rios autom√°ticos de estat√≠sticas e qualidade dos dados (profiling) para documenta√ß√£o e an√°lise. |
| **Roberto Arruda** | **Cientista de Dados (Ingest√£o e Modelagem)** |  Realizou a ingest√£o inicial dos dados, estruturando o pipeline nas camadas Bronze, Silver e Gold. Foi respons√°vel pela organiza√ß√£o da arquitetura de pastas, padroniza√ß√£o do fluxo de dados e pela cria√ß√£o das primeiras transforma√ß√µes entre as camadas. |


***

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa [MIT](https://opensource.org/licenses/MIT).
