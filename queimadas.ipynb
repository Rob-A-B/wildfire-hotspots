{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rob-A-B/wildfire-hotspots/blob/main/queimadas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, datetime\n",
        "from google.colab import files\n",
        "import shutil, os, datetime\n",
        "import os, glob, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from google.colab import drive\n",
        "import gc"
      ],
      "metadata": {
        "id": "CRzpcNEw_gSm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aTF7o-820tZ",
        "outputId": "f64ce4f3-613e-4bfa-ba2d-c7ce91b16614"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hoje = datetime.date.today()\n",
        "BASE   = \"/content/projeto_pcb\"\n",
        "BRONZE_RAW = f\"{BASE}/dados/bronze/raw\"                   # onde guardamos os arquivos \"brutos\" por partiÃ§Ã£o\n",
        "BRONZE_CUR = f\"{BASE}/dados/bronze/current\"               # dataset consolidado (Ãºnico arquivo)\n",
        "os.makedirs(BRONZE_RAW, exist_ok=True)\n",
        "os.makedirs(BRONZE_CUR, exist_ok=True)\n",
        "\n",
        "# comentario teste de commit\n",
        "\n",
        "# Define the path to your CSV files in Google Drive\n",
        "# **IMPORTANT:** Replace 'My Drive/Your_Folder_With_CSVs' with the actual path to your folder\n",
        "DRIVE_CSV_PATH = \"/content/drive/My Drive/8_periodo/Big Data/datasets\"\n",
        "os.makedirs(DRIVE_CSV_PATH, exist_ok=True) # Create the directory if it doesn't exist"
      ],
      "metadata": {
        "id": "gcG1smU_2tnE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, datetime\n",
        "from google.colab import files\n",
        "import shutil, os, datetime\n",
        "import os, glob, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from google.colab import drive\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "hoje = datetime.date.today()\n",
        "BASE   = \"/content/projeto_pcb\"\n",
        "BRONZE_RAW = f\"{BASE}/dados/bronze/raw\"                   # onde guardamos os arquivos \"brutos\" por partiÃ§Ã£o\n",
        "BRONZE_CUR = f\"{BASE}/dados/bronze/current\"               # dataset consolidado (Ãºnico arquivo)\n",
        "os.makedirs(BRONZE_RAW, exist_ok=True)\n",
        "os.makedirs(BRONZE_CUR, exist_ok=True)\n",
        "\n",
        "# Define the path to your CSV files in Google Drive\n",
        "# **IMPORTANT:** Replace 'My Drive/Your_Folder_With_CSVs' with the actual path to your folder\n",
        "DRIVE_CSV_PATH = \"/content/drive/My Drive/8_periodo/Big Data/datasets\" # Assuming this is the correct path from previous execution\n",
        "os.makedirs(DRIVE_CSV_PATH, exist_ok=True) # Create the directory if it doesn't exist\n",
        "\n",
        "\n",
        "# 1) Descobrir todos os Parquets diretamente em /content/drive/My Drive/Your_Folder_With_Parquets (Assuming you have converted CSVs to Parquet)\n",
        "# NOTE: If your files are still CSV, you'll need to convert them to Parquet first.\n",
        "arquivos_parquet = [f for f in os.listdir(DRIVE_CSV_PATH) if f.endswith(\".parquet\") and os.path.isfile(f\"{DRIVE_CSV_PATH}/{f}\")]\n",
        "print(f\"ðŸ“¦ Encontrados {len(arquivos_parquet)} Parquet em {DRIVE_CSV_PATH}.\\n\")\n",
        "\n",
        "# 2) Mover/copy to Bronze/raw partitioned and mount list of DataFrames\n",
        "dfs = []\n",
        "for nome in arquivos_parquet:\n",
        "    caminho_src = f\"{DRIVE_CSV_PATH}/{nome}\"\n",
        "\n",
        "    # extrai AAAAMM do nome (ex.: focos_mensal_br_202311.parquet)\n",
        "    m = re.search(r'(\\d{6})', nome)\n",
        "    aaaamm = m.group(1) if m else f\"{hoje.year}{hoje:%m}\"\n",
        "    ano, mes = aaaamm[:4], aaaamm[4:]\n",
        "\n",
        "    # destino particionado\n",
        "    dest_dir = os.path.join(BRONZE_RAW, f\"ano={ano}\", f\"mes={mes}\")\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    caminho_dst = os.path.join(dest_dir, nome)\n",
        "\n",
        "    # Copy (to keep the original in Drive)\n",
        "    shutil.copy(caminho_src, caminho_dst)\n",
        "    print(f\"âœ… Copiado: {nome}  âžœ  {caminho_dst}\")\n",
        "\n",
        "\n",
        "# 3) Consolidate everything into a single Dask DataFrame (Bronze/current)\n",
        "# Read all parquet files from the raw bronze directory into a single Dask DataFrame\n",
        "bronze_ddf = dd.read_parquet(os.path.join(BRONZE_RAW, \"*/*/*.parquet\"))\n",
        "\n",
        "# Simple idempotence: remove exact duplicate rows\n",
        "# (If desired, change to subset=[\"id\",\"datahora\",\"latitude\",\"longitude\"] if they exist)\n",
        "bronze_ddf = bronze_ddf.drop_duplicates()\n",
        "\n",
        "print(f\"\\nðŸ§± Bronze consolidated: {len(bronze_ddf):,} linhas, {bronze_ddf.shape[1]} colunas\")\n",
        "# For Dask, we use .head() to see the first few rows. This triggers computation.\n",
        "display(bronze_ddf.head())\n",
        "\n",
        "# 4) Salva consolidado (Parquet)\n",
        "# We will save the consolidated data as a single Parquet file for efficiency with Dask\n",
        "out_parquet = os.path.join(BRONZE_CUR, \"focos_bronze_consolidado.parquet\")\n",
        "\n",
        "# To save as a single file, we repartition to 1 partition before saving\n",
        "bronze_ddf.repartition(npartitions=1).to_parquet(out_parquet)\n",
        "\n",
        "\n",
        "print(f\"\\nðŸ’¾ Consolidado salvo em:\\n - {out_parquet}\")"
      ],
      "metadata": {
        "id": "1DRL3jQbwBNc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "1d964e28-341b-4d57-a717-64722d3837c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Encontrados 34 CSVs em /content/drive/My Drive/8_periodo/Big Data/datasets.\n",
            "\n",
            "âœ… Copiado: focos_mensal_br_202504.csv  âžœ  /content/projeto_pcb/dados/bronze/raw/ano=2025/mes=04/focos_mensal_br_202504.csv\n",
            "âœ… Copiado: focos_mensal_br_202501.csv  âžœ  /content/projeto_pcb/dados/bronze/raw/ano=2025/mes=01/focos_mensal_br_202501.csv\n",
            "âœ… Copiado: focos_mensal_br_202502.csv  âžœ  /content/projeto_pcb/dados/bronze/raw/ano=2025/mes=02/focos_mensal_br_202502.csv\n",
            "âœ… Copiado: focos_mensal_br_202503.csv  âžœ  /content/projeto_pcb/dados/bronze/raw/ano=2025/mes=03/focos_mensal_br_202503.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2727149867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Copy (to keep the original in Drive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaminho_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaminho_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Copiado: {nome}  âžœ  {caminho_dst}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f5348a7",
        "outputId": "12af8cf3-9cd3-440a-d901-73b369e0d03e"
      },
      "source": [
        "!pip install dask dask[dataframe] pyarrow"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.12/dist-packages (2025.5.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask) (8.3.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from dask) (25.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask) (6.0.3)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.12/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.head()"
      ],
      "metadata": {
        "id": "XqmjkB3I4g8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.info()"
      ],
      "metadata": {
        "id": "w-4R5Al_1Vhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.isna().sum()"
      ],
      "metadata": {
        "id": "uE1COQJI1b1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hoje = datetime.date.today()\n",
        "BASE = \"/content/projeto_pcb\"\n",
        "BRONZE = \"/content/projeto_pcb/dados/bronze/current\"\n",
        "SILVER = \"/content/projeto_pcb/dados/silver\"\n",
        "os.makedirs(SILVER, exist_ok=True)"
      ],
      "metadata": {
        "id": "2aM4FxeB4B8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = None\n",
        "del df_temp\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "iK1kcZPf8I1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.drop(columns=['id', 'lat','lon','municipio_id','estado_id','pais_id','pais','origem_arquivo', 'ano', 'mes'], inplace=True)"
      ],
      "metadata": {
        "id": "EWYQNJq79aPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "paths = sorted(glob.glob(os.path.join(BRONZE, \"*.csv\")))\n",
        "assert paths, \"Nenhum CSV encontrado em bronze.\"\n",
        "\n",
        "#LEMBRAR dessa linha mais na frente quando tiver mais meses / tratar isso no bronze\n",
        "#df = pd.concat((pd.read_csv(p) for p in paths), ignore_index=True)\n",
        "\n",
        "esperadas = {\"data_hora_gmt\",\"satelite\",\"municipio\",\"estado\"}\n",
        "faltando = esperadas - set(bronze_df.columns)\n",
        "assert not faltando, f\"Faltam colunas no CSV: {faltando}\"\n",
        "\n",
        "bronze_df[\"data_hora_gmt\"] = pd.to_datetime(bronze_df[\"data_hora_gmt\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "for c in [\"satelite\",\"municipio\",\"estado\"]:\n",
        "    bronze_df[c] = bronze_df[c].astype(str).str.strip()\n",
        "\n",
        "\n",
        "bronze_df = bronze_df.dropna(subset=[\"data_hora_gmt\"]).copy()\n",
        "\n",
        "# Identify and replace -999 with NaN in numeric columns\n",
        "numeric_cols = bronze_df.select_dtypes(include=np.number).columns\n",
        "bronze_df[numeric_cols] = bronze_df[numeric_cols].replace(-999, np.nan)\n",
        "\n",
        "# Calculate and print proportion of NaNs per column\n",
        "nan_proportion_per_column = bronze_df.isnull().sum() / len(bronze_df) * 100\n",
        "print(\"\\nProporÃ§Ã£o de NaN por coluna (%):\")\n",
        "print(nan_proportion_per_column)\n",
        "\n",
        "# Calculate and print proportion of rows with NaN\n",
        "rows_with_nan = bronze_df.isnull().any(axis=1).sum()\n",
        "proportion_rows_with_nan = rows_with_nan / len(bronze_df) * 100\n",
        "print(f\"\\nProporÃ§Ã£o de linhas com NaN no total do dataset (%): {proportion_rows_with_nan:.2f}%\")\n",
        "\n",
        "\n",
        "# Drop rows with NaN values\n",
        "bronze_df = bronze_df.dropna().copy()\n",
        "\n",
        "\n",
        "bronze_df[\"ano\"] = bronze_df[\"data_hora_gmt\"].dt.year.astype(\"Int64\")\n",
        "bronze_df[\"mes\"] = bronze_df[\"data_hora_gmt\"].dt.month.astype(\"Int64\")\n",
        "bronze_df[\"dia\"] = bronze_df[\"data_hora_gmt\"].dt.day.astype(\"Int64\")\n",
        "\n",
        "# carga que prof pediu\n",
        "bronze_df[\"dt_carga\"] = datetime.date.today().isoformat()\n",
        "\n",
        "# Salva silver\n",
        "silver_csv = os.path.join(SILVER, f\"focos_silver_{datetime.date.today():%Y%m%d}.csv\")\n",
        "os.makedirs(SILVER, exist_ok=True)\n",
        "bronze_df.to_csv(silver_csv, index=False)\n",
        "\n",
        "print(\"Silver salvo:\", silver_csv, \"| linhas:\", len(bronze_df))\n",
        "bronze_df.head()"
      ],
      "metadata": {
        "id": "4msZhIXbeoEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "yLOx2LLhEBEU",
        "outputId": "ba244e9b-0285-4d71-bcbd-8178752290ca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data_hora_gmt                 0\n",
              "satelite                      0\n",
              "municipio                     0\n",
              "estado                        0\n",
              "numero_dias_sem_chuva    302472\n",
              "precipitacao             302472\n",
              "risco_fogo               302472\n",
              "bioma                         9\n",
              "frp                      511105\n",
              "ano                           0\n",
              "mes                           0\n",
              "dia                           0\n",
              "dt_carga                      0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>data_hora_gmt</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>satelite</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>municipio</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>estado</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numero_dias_sem_chuva</th>\n",
              "      <td>302472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precipitacao</th>\n",
              "      <td>302472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>risco_fogo</th>\n",
              "      <td>302472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bioma</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>frp</th>\n",
              "      <td>511105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ano</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dia</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dt_carga</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bronze()"
      ],
      "metadata": {
        "id": "_iaBgve1EE2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOLD = \"/content/projeto_pcb/dados/gold\n",
        "os.makedirs(GOLD, exist_ok=True)"
      ],
      "metadata": {
        "id": "jRgOS1ml7Wcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padrao_silver = \"focos_silver_*.csv\"   # ajuste se usou outro nome\n",
        "\n",
        "paths = sorted(glob.glob(os.path.join(SILVER, padrao_silver)))\n",
        "assert paths, f\"Nenhum CSV Silver encontrado em: {SILVER}\"\n",
        "\n",
        "# parse_dates  reforÃ§andos a conversÃ£o em seguida\n",
        "df = pd.concat((pd.read_csv(p, parse_dates=[\"data_hora_gmt\"]) for p in paths),\n",
        "               ignore_index=True)\n",
        "\n",
        "# Garante que data_hora_gmt Ã© datetime\n",
        "df[\"data_hora_gmt\"] = pd.to_datetime(df[\"data_hora_gmt\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "# NormalizaÃ§Ãµes Ãºteis para agregaÃ§Ã£o\n",
        "df[\"data\"] = df[\"data_hora_gmt\"].dt.date\n",
        "df[\"ano\"]  = df[\"data_hora_gmt\"].dt.year\n",
        "df[\"mes\"]  = df[\"data_hora_gmt\"].dt.month\n",
        "df[\"hora\"] = df[\"data_hora_gmt\"].dt.hour\n",
        "\n",
        "por_estado_mes = (\n",
        "    df.groupby([\"estado\",\"ano\",\"mes\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"qtd_focos\")\n",
        "      .sort_values([\"ano\",\"mes\",\"qtd_focos\"], ascending=[True,True,False])\n",
        ")\n",
        "\n",
        "\n",
        "por_municipio_dia = (\n",
        "    df.groupby([\"estado\",\"municipio\",\"data\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"qtd_focos\")\n",
        "      .sort_values([\"data\",\"qtd_focos\"], ascending=[True,False])\n",
        ")\n",
        "\n",
        "\n",
        "por_satelite_mes = (\n",
        "    df.groupby([\"satelite\",\"ano\",\"mes\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"qtd_focos\")\n",
        "      .sort_values([\"ano\",\"mes\",\"qtd_focos\"], ascending=[True,True,False])\n",
        ")\n",
        "\n",
        "por_satelite_total = (\n",
        "    df.groupby(\"satelite\")\n",
        "      .size()\n",
        "      .reset_index(name=\"qtd_focos\")\n",
        "      .assign(perc=lambda x: (x[\"qtd_focos\"] / x[\"qtd_focos\"].sum())*100)\n",
        "      .sort_values(\"qtd_focos\", ascending=False)\n",
        "      .round({\"perc\": 2})\n",
        ")\n",
        "\n",
        "df[\"lat_cell\"] = df[\"lat\"].round(2)\n",
        "df[\"lon_cell\"] = df[\"lon\"].round(2)\n",
        "grade_espacial = (\n",
        "    df.groupby([\"lat_cell\",\"lon_cell\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"qtd_focos\")\n",
        "      .sort_values(\"qtd_focos\", ascending=False)\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Salva em GOLD\n",
        "# ------------------------------\n",
        "os.makedirs(GOLD, exist_ok=True)\n",
        "hoje = datetime.date.today().isoformat()\n",
        "\n",
        "g1 = os.path.join(GOLD, f\"gold_focos_por_estado_mes_{hoje}.csv\")\n",
        "g2 = os.path.join(GOLD, f\"gold_focos_por_municipio_dia_{hoje}.csv\")\n",
        "g3 = os.path.join(GOLD, f\"gold_focos_por_satelite_mes_{hoje}.csv\")\n",
        "g3b= os.path.join(GOLD, f\"gold_focos_por_satelite_total_{hoje}.csv\")\n",
        "g4 = os.path.join(GOLD, f\"gold_grade_espacial_{hoje}.csv\")\n",
        "\n",
        "por_estado_mes.to_csv(g1, index=False)\n",
        "por_municipio_dia.to_csv(g2, index=False)\n",
        "por_satelite_mes.to_csv(g3, index=False)\n",
        "por_satelite_total.to_csv(g3b, index=False)\n",
        "grade_espacial.to_csv(g4, index=False)\n",
        "\n",
        "print(\"Gold salvo:\")\n",
        "print(\" -\", g1)\n",
        "print(\" -\", g2)\n",
        "print(\" -\", g3)\n",
        "print(\" -\", g3b)\n",
        "print(\" -\", g4)\n",
        "\n",
        "try:\n",
        "    ult_ano  = por_estado_mes[\"ano\"].max()\n",
        "    ult_mes  = por_estado_mes.query(\"ano == @ult_ano\")[\"mes\"].max()\n",
        "    top_estados = (\n",
        "        por_estado_mes\n",
        "        .query(\"ano == @ult_ano and mes == @ult_mes\")\n",
        "        .nlargest(10, \"qtd_focos\")\n",
        "    )\n",
        "    print(f\"\\nTOP estados no mÃªs mais recente ({ult_ano}-{ult_mes:02d}):\")\n",
        "    print(top_estados.to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"\\n[Aviso] NÃ£o foi possÃ­vel imprimir TOP estados do mÃªs mais recente:\", e)\n",
        "\n",
        "print(\"\\n% por satÃ©lite (total):\")\n",
        "print(por_satelite_total.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No3rv5KVzVuu",
        "outputId": "e7601068-1215-4b74-cdb9-e8ad68bf9016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold salvo:\n",
            " - /content/projeto_pcb/dados/gold/gold_focos_por_estado_mes_2025-10-09.csv\n",
            " - /content/projeto_pcb/dados/gold/gold_focos_por_municipio_dia_2025-10-09.csv\n",
            " - /content/projeto_pcb/dados/gold/gold_focos_por_satelite_mes_2025-10-09.csv\n",
            " - /content/projeto_pcb/dados/gold/gold_focos_por_satelite_total_2025-10-09.csv\n",
            " - /content/projeto_pcb/dados/gold/gold_grade_espacial_2025-10-09.csv\n",
            "\n",
            "TOP estados no mÃªs mais recente (2024-03):\n",
            "            estado  ano  mes  qtd_focos\n",
            "           RORAIMA 2024    3       1767\n",
            "       MATO GROSSO 2024    3       1508\n",
            "             BAHIA 2024    3        936\n",
            "MATO GROSSO DO SUL 2024    3        562\n",
            "             GOIÃS 2024    3        429\n",
            "         SÃƒO PAULO 2024    3        282\n",
            "      MINAS GERAIS 2024    3        260\n",
            "         TOCANTINS 2024    3        132\n",
            "              PARÃ 2024    3         98\n",
            "           ALAGOAS 2024    3         93\n",
            "\n",
            "% por satÃ©lite (total):\n",
            " satelite  qtd_focos  perc\n",
            "  NOAA-20       1870 28.18\n",
            "  NPP-375       1399 21.08\n",
            " NPP-375D        883 13.31\n",
            "  GOES-16        871 13.13\n",
            " AQUA_M-T        593  8.94\n",
            "TERRA_M-T        465  7.01\n",
            "  METOP-B        229  3.45\n",
            "TERRA_M-M        154  2.32\n",
            "  METOP-C        131  1.97\n",
            " AQUA_M-M         28  0.42\n",
            "   MSG-03         13  0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cont = df[\"Grey\"].value_counts().sort_index()\n",
        "# plt.bar(cont.index.astype(str), cont.values)\n",
        "# plt.title(\"DistribuiÃ§Ã£o da classe Grey\")\n",
        "# plt.xlabel(\"Grey (0=nÃ£o cinza, 1=cinza)\")\n",
        "# # plt.ylabel(\"n\")\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "zoIrgibL1j5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agg = df.groupby(\"Grey\")[[\"R\",\"G\",\"B\"]].mean().reset_index()\n",
        "# for ch in [\"R\",\"G\",\"B\"]:\n",
        "#     plt.bar(agg[\"Grey\"].astype(str), agg[ch], label=ch)\n",
        "#     plt.title(f\"MÃ©dia do canal {ch} por Grey\")\n",
        "#     plt.xlabel(\"Grey\")\n",
        "#     plt.ylabel(f\"MÃ©dia {ch} (escala {'0â€“255' if df['R'].max()>1 else '0â€“1'})\")\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "id": "O0u7a6p712k8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}